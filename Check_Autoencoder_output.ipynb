{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "548855af-53e2-4c33-bbc0-c67a9ffb9b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf12dbf-2b44-4f2e-8264-885d74b8a3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder = torch.load('./final_encoder_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdeb9dcb-b815-4808-b728-4d12a782cd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "batch_size = 128\n",
    "data_dir = \"/home/jupyter\"\n",
    "transforms = v2.Compose([\n",
    "v2.ToImage(),  # Convert to tensor, only needed if you had a PIL image\n",
    "v2.ToDtype(torch.uint8),  # optional, most input are already uint8 at this point\n",
    "v2.ToTensor(),\n",
    "v2.RandomApply(transforms=[v2.RandomResizedCrop(size=(32, 32), scale = (0.9,0.9),antialias = True),\n",
    "                               #v2.RandomRotation(degrees=(5,10)),\n",
    "                               v2.GaussianBlur(kernel_size=(5,5), sigma=1),\n",
    "                               v2.ColorJitter(brightness=0.5)  \n",
    "                               #v2.RandomPerspective(p = 1),  #default distortion is 0.5\n",
    "                               #v2.RandomAdjustSharpness(sharpness_factor = 2, p = 1)  #double the sharpness\n",
    "                              ], p=0.8),\n",
    "v2.ConvertImageDtype(torch.float32),\n",
    "v2.Normalize((0.5,),(0.5,))])\n",
    "test_dataset = datasets.ImageFolder(root=data_dir+'/test/', transform=transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=10000,\n",
    "                                        num_workers=num_workers)\n",
    "\n",
    "\n",
    "num_workers = 2\n",
    "batch_size = 128\n",
    "data_dir = \"/home/jupyter\"\n",
    "transforms = v2.Compose([\n",
    "v2.ToImage(),  # Convert to tensor, only needed if you had a PIL image\n",
    "v2.ToDtype(torch.uint8),  # optional, most input are already uint8 at this point\n",
    "v2.ToTensor(),\n",
    "v2.ConvertImageDtype(torch.float32)])\n",
    "test_dataset = datasets.ImageFolder(root=data_dir+'/test/', transform=transforms)\n",
    "test_loader_notransform = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                        batch_size=10000,\n",
    "                                                        num_workers=num_workers)\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for augmented, _ in test_loader: \n",
    "        augmented = augmented.to(device='cuda', dtype=torch.float32)\n",
    "        reconstructed_augmented = autoencoder.forward(augmented)\n",
    "        autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for original, _ in test_loader_notransform: \n",
    "        original = original.to(device='cuda', dtype=torch.float32)\n",
    "        reconstructed_original = autoencoder.forward(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "427e7301-0775-43ab-9f9b-3642c174974b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for augmented, _ in test_loader: \n",
    "        augmented = augmented.to(device='cuda', dtype=torch.float32)\n",
    "        reconstructed_augmented = autoencoder.forward(augmented)\n",
    "        autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for original, _ in test_loader_notransform: \n",
    "        original = original.to(device='cuda', dtype=torch.float32)\n",
    "        reconstructed_original = autoencoder.forward(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ffa567d-2014-4f37-a9d0-8553f545e120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
