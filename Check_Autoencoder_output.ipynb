{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc350fa-af64-46d0-9338-756c3a20a101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb as wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf12dbf-2b44-4f2e-8264-885d74b8a3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "class Conv_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_c, channel_1, channel_2, hidden_dim):\n",
    "        super(Conv_Autoencoder, self).__init__()\n",
    "        self.channel_2 = channel_2\n",
    "        self.encoder = nn.Sequential(\n",
    "                                    nn.Conv2d(3, channel_1, kernel_size=3, stride = 2, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(channel_1, channel_2, kernel_size=3, stride = 2, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    Flatten(),\n",
    "                                    nn.Linear(channel_2*8*8, hidden_dim),\n",
    "                                    nn.ReLU())\n",
    "        self.linear = nn.Sequential(nn.Linear(hidden_dim, channel_2*8*8))\n",
    "        self.decoder = nn.Sequential(\n",
    "                                    nn.Upsample(scale_factor = 2, mode = \"bilinear\"),\n",
    "                                    nn.ConvTranspose2d(channel_2, channel_1, kernel_size = 3, stride=1, padding = 1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Upsample(scale_factor = 2, mode = \"bilinear\"),\n",
    "                                    nn.ConvTranspose2d(channel_1, input_c, kernel_size = 3, stride=1, padding = 1),\n",
    "                                    nn.Tanh())\n",
    "    def forward(self, x): \n",
    "        hidden_rep = self.encoder(x)\n",
    "        self.hidden_rep = hidden_rep\n",
    "        rev_linear = self.linear(self.hidden_rep)\n",
    "        rev_linear = rev_linear.reshape([rev_linear.shape[0], self.channel_2, 8, 8])\n",
    "        reconstructed = self.decoder(rev_linear)\n",
    "        return hidden_rep\n",
    "    \n",
    "autoencoder = torch.load('./final_encoder_model.pt')\n",
    "autoencoder.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdeb9dcb-b815-4808-b728-4d12a782cd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "batch_size = 128\n",
    "data_dir = \"/home/jupyter\"\n",
    "transforms = v2.Compose([\n",
    "v2.ToImage(),  # Convert to tensor, only needed if you had a PIL image\n",
    "v2.ToDtype(torch.uint8),  # optional, most input are already uint8 at this point\n",
    "v2.ToTensor(),\n",
    "v2.RandomApply(transforms=[v2.RandomResizedCrop(size=(32, 32), scale = (0.9,0.9),antialias = True),\n",
    "                               #v2.RandomRotation(degrees=(5,10)),\n",
    "                               v2.GaussianBlur(kernel_size=(5,5), sigma=1),\n",
    "                               v2.ColorJitter(brightness=0.5)  \n",
    "                               #v2.RandomPerspective(p = 1),  #default distortion is 0.5\n",
    "                               #v2.RandomAdjustSharpness(sharpness_factor = 2, p = 1)  #double the sharpness\n",
    "                              ], p=0.8),\n",
    "v2.ConvertImageDtype(torch.float32),\n",
    "v2.Normalize((0.5,),(0.5,))])\n",
    "test_dataset = datasets.ImageFolder(root=data_dir+'/test/', transform=transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=10000,\n",
    "                                        num_workers=num_workers)\n",
    "\n",
    "\n",
    "num_workers = 2\n",
    "batch_size = 128\n",
    "data_dir = \"/home/jupyter\"\n",
    "transforms = v2.Compose([\n",
    "v2.ToImage(),  # Convert to tensor, only needed if you had a PIL image\n",
    "v2.ToDtype(torch.uint8),  # optional, most input are already uint8 at this point\n",
    "v2.ToTensor(),\n",
    "test_dataset = datasets.ImageFolder(root=data_dir+'/test/', transform=transforms)\n",
    "test_loader_notransform = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                        batch_size=10000,\n",
    "                                                        num_workers=num_workers)\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for augmented, _ in test_loader: \n",
    "        augmented = augmented.to(device='cuda', dtype=torch.float32)\n",
    "        print(torch.min(augmented))\n",
    "        print(torch.max(augmented))\n",
    "        reconstructed_augmented = autoencoder.forward(augmented)\n",
    "        autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for original, _ in test_loader_notransform: \n",
    "        original = original.to(device='cuda', dtype=torch.float32)\n",
    "        reconstructed_original = autoencoder.forward(original)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
